{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8122c262",
      "metadata": {
        "id": "8122c262",
        "papermill": {
          "duration": 0.012564,
          "end_time": "2023-12-10T19:30:49.455969",
          "exception": false,
          "start_time": "2023-12-10T19:30:49.443405",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "<div style=\"align-items: center; justify-content: space-between;\">\n",
        "   \n",
        "   <h2>Data Pipeline Telegram</h2>\n",
        "   <b> Nome: <a href=\"https://www.linkedin.com/in/fernandohcarneiro/\">Ederson Santos</a></b>\n",
        "   <br><br>\n",
        "   <img src=\"https://raw.githubusercontent.com/edersonss1987/AutomacaoTelegram/57fd76228b42a7e5c556113975d0ad5be782693e/imagens/Fonte%20dos%20dados.png\"  align=\"center\" alt=\"data-pipeline\" width=\"auto\">\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d78f1e07",
      "metadata": {
        "id": "d78f1e07",
        "papermill": {
          "duration": 0.011935,
          "end_time": "2023-12-10T19:30:49.48017",
          "exception": false,
          "start_time": "2023-12-10T19:30:49.468235",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "\n",
        "## Sumário\n",
        "#### 1. [**Introdução**](#intro)  \n",
        "    1.1 Objetivo  \n",
        "    1.2 O que é Pipeline?  \n",
        "    1.3 Pipeline do projeto  \n",
        "---\n",
        "#### 2. [**Etapa transacional**](#sistrans)  \n",
        "    2.1 Ingestão de dados  \n",
        "    2.2 O que é uma API?  \n",
        "    2.3 Telegram Botfather  \n",
        "    2.4 Webhook  \n",
        "    2.5 AWS API Gateway  \n",
        "---\n",
        "#### 3. [**Etapa analítico**](#sisanal)  \n",
        "    3.1 O que é ETL (Extraction, Transformation, Loading)?  \n",
        "    3.2 Extração    \n",
        "        3.2.1 AWS Lambda  \n",
        "        3.2.2 AWS S3\n",
        "        3.2.3 Extração Telegram   \n",
        "    3.3 Transformação\n",
        "    3.4 Carregamento    \n",
        "---\n",
        "#### 4. [**Apresentação**](#apres)  \n",
        "    4.1 AWS Athena  \n",
        "    4.2 Análise de Dados\n",
        "#### 5. [**Conclusão**](#conclu)  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daac4784",
      "metadata": {
        "id": "daac4784",
        "papermill": {
          "duration": 0.011658,
          "end_time": "2023-12-10T19:30:49.504034",
          "exception": false,
          "start_time": "2023-12-10T19:30:49.492376",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "<a id='intro'></a>\n",
        "## 1. Introdução"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad6f1017",
      "metadata": {
        "id": "ad6f1017",
        "papermill": {
          "duration": 0.012188,
          "end_time": "2023-12-10T19:30:49.52849",
          "exception": false,
          "start_time": "2023-12-10T19:30:49.516302",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### 1.1 Objetivo\n",
        "O objetivo deste projeto é apresentar o processo de extração de dados do Telegram, em sua forma original em um formaro semi-estruturado(.JSON),transferir esses dados para um ***Datalake***, realizar o processamento dos dados de forma automatizada e então fazer a análise dos dados tratados, usando linguagem SQL. Esses dados fornecem a base para extrair informações valiosas, abrindo possibilidades para aprimorar serviços e explorar oportunidades.\n",
        "\n",
        "### 1.2 O que é Pipeline?\n",
        "A palavra “pipeline” em inglês é frequentemente usada em português com o mesmo significado, especialmente em argumentos técnicos ou de negócios. No entanto ele é muito utilizado em um contexto de processamento de dados ou de projetos, pode ser traduzida como “fluxo de trabalho” ou “processo”.\n",
        "\n",
        "Um Pipeline de Dados é uma série de etapas de processamento de dados, onde a saída de uma etapa é a entrada para a próxima. Essas etapas podem incluir coleta, limpeza, transformação, modelagem e visualização de dados.\n",
        "\n",
        "### 1.3 Pipeline do projeto\n",
        "O pipeline de dados deste projeto inicia com a ingestão dos dados de um aplicavo de mensagens, bastante conhecido *TELEGRAM*, as trocas de mensagens são coletadas via *WEBHOOK*, conectada a *API* da *Amazon Web Services (AWS)*. Na plataforma da *AWS*, os dados são recebidos por uma função *Lambda*, que os organiza por dias no *AWS S3*. Diariamente, um AWS *Event Bridge* aciona um processo em lote no serviço *Lambda*, que transforma os dados brutos, extrai apenas as informações relevantes (data da mensagem,user_id, nome do contato,texto digitado e tipo de chat(grupo)) e os armazena de maneira organizada no *AWS S3*. No processo de visualização, tabelas criadas a partir dos arquivos *Parquet* gerados no passo anterior possibilitam a realização de análises variadas usando a linguagem *SQL*.\n",
        "\n",
        "![Pipeline Telegram](https://raw.githubusercontent.com/edersonss1987/AutomacaoTelegram/57fd76228b42a7e5c556113975d0ad5be782693e/imagens/Fonte%20dos%20dados.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aac5225a",
      "metadata": {
        "id": "aac5225a",
        "papermill": {
          "duration": 0.011947,
          "end_time": "2023-12-10T19:30:49.55312",
          "exception": false,
          "start_time": "2023-12-10T19:30:49.541173",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "<a id='sistrans'></a>\n",
        "## 2. Sistema Transacional"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d3dfb03",
      "metadata": {
        "id": "0d3dfb03",
        "papermill": {
          "duration": 0.011732,
          "end_time": "2023-12-10T19:30:49.57694",
          "exception": false,
          "start_time": "2023-12-10T19:30:49.565208",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Um sistema transacional é uma estrutura tecnológica e de dados que suporta as transações diárias de uma fonte de dados. Ele é responsável por coletar, registrar e transmitir dados para uma cadeia de processos que tratarão os dados para posterior análise. Em resumo, os sistemas transacionais são projetados para ingerir dados criados diariamente e salvá-los em um banco de dados ou *DataLake*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1201766",
      "metadata": {
        "id": "b1201766",
        "papermill": {
          "duration": 0.011657,
          "end_time": "2023-12-10T19:30:49.600681",
          "exception": false,
          "start_time": "2023-12-10T19:30:49.589024",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### 2.1 Ingestão de Dados\n",
        "Neste projeto a ingestão dos dados consiste na captura de mensagens de texto enviadas pelas plataformas Whatsapp e Telegram em tempo real, fornecidas por meio de uma API."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5da995c",
      "metadata": {
        "id": "d5da995c",
        "papermill": {
          "duration": 0.011708,
          "end_time": "2023-12-10T19:30:49.624671",
          "exception": false,
          "start_time": "2023-12-10T19:30:49.612963",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "#### 2.2 O que é uma API?\n",
        "Uma API, ou Interface de Programação de Aplicações, é um componente de software dentro de um sistema que fornece um mecanismo para invocar uma tarefa em outro sistema. Ela serve como uma ponte que facilita a comunicação e integração entre sistemas ou componentes de software diversos, até de tipos diferentes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2b82b64",
      "metadata": {
        "id": "d2b82b64",
        "papermill": {
          "duration": 0.011723,
          "end_time": "2023-12-10T19:30:49.698205",
          "exception": false,
          "start_time": "2023-12-10T19:30:49.686482",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "#### 2.3 AWS API Gateway\n",
        "O *API Gateway* é o primeiro serviço em nuvem na sequência do pipeline, faz parte dos serviços em nuvem da *Amazon Web Services (AWS)*. Este serviço centraliza o gerenciamento das APIs inclusive o recebimento, como no caso deste projeto, em que servirá como porta de entrada para o recebimento do *payload* advindo das plataformas de mensagens. Ele oferece recursos como autenticação, autorização, monitoramento e escalabilidade, simplificando o processo de construção e administração de APIs de forma eficiente na infraestrutura da AWS.\n",
        "\n",
        "\n",
        "\n",
        "##### 2.3.1 Criação da API para Telegram\n",
        "Na plataforma *Telegram*, foi criado uma nova API com protocolo REST com método POST, configurado com integração do tipo proxy com o serviço Lambda.\n",
        "\n",
        "<img src=\"https://github.com/edersonss1987/AutomacaoTelegram/blob/main/imagens/API%20Telegram.png?raw=true\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f32904d",
      "metadata": {
        "id": "6f32904d",
        "papermill": {
          "duration": 0.011673,
          "end_time": "2023-12-10T19:30:49.722071",
          "exception": false,
          "start_time": "2023-12-10T19:30:49.710398",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "#### 2.4 Webhook\n",
        "Um Webhook é um método de comunicação entre sistemas na web que permite a um sistema notificar outro quando um evento específico ocorre. Ele funciona como uma “chamada de retorno” HTTP: um servidor envia uma solicitação HTTP para a URL de um webhook configurado em outro sistema quando o evento ocorre. Isso permite que o sistema receptor seja atualizado ou tome uma ação imediatamente após o evento. É amplamente utilizado para integrações e automações entre diferentes serviços web.\n",
        "\n",
        "\n",
        "##### 2.4.1 Configuração do Telegram\n",
        "O procedimento na plataforma *Telegram* foi preciso acionar o método `setWebhook`. Abaixo segue a captura da execução da requisição:\n",
        "```python\n",
        "requests.get(f'https://api.telegram.org/bot{token}/setWebhook?url={base_url}')\n",
        "```\n",
        "\n",
        "<img src= \"https://github.com/edersonss1987/AutomacaoTelegram/blob/main/imagens/setWebhook.png?raw=true\">\n",
        "\n",
        "##### 2.5.1 Configuração do Telegram\n",
        "\n",
        "\n",
        "Após configurado o *webhook* foi possível verificar seu funcionamento com o método `getWebhookInfo`:\n",
        "\n",
        "<img src=\"https://github.com/edersonss1987/AutomacaoTelegram/blob/main/imagens/getWebhootInfo.png?raw=true\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f101b6a",
      "metadata": {
        "id": "1f101b6a",
        "papermill": {
          "duration": 0.012283,
          "end_time": "2023-12-10T19:30:49.747332",
          "exception": false,
          "start_time": "2023-12-10T19:30:49.735049",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "<a id='sisanal'></a>\n",
        "## 3. Sistema analítico\n",
        "Esses sistemas apoiam a tomada de decisões, relatórios, consultas e análises. São projetados para lidar com consultas complexas em grandes volumes de dados vindos dos sistemas transacionais, organizam esses dados e os processam de maneira a criar insights úteis. Neste projeto o sistema compreende a retirada dos dados brutos (*raw*) do datalake, a transformação deles em informação e a análise em busca de padrões e insights.\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "{\n",
        " \"ok\": true,\n",
        " \"result\": [\n",
        "  {\n",
        "   \"update_id\": 126178061,\n",
        "   \"message\": {\n",
        "    \"message_id\": 282,\n",
        "    \"from\": {\n",
        "     \"id\": 6310508671,\n",
        "     \"is_bot\": false,\n",
        "     \"first_name\": \"Eder ControliD\",\n",
        "     \"language_code\": \"pt-br\"\n",
        "    },\n",
        "    \"chat\": {\n",
        "     \"id\": -1002096546090,\n",
        "     \"title\": \"Eder_IDBot\",\n",
        "     \"type\": \"supergroup\"\n",
        "    },\n",
        "    \"date\": 1715387518,\n",
        "    \"text\": \"Mensagens capturadas pelo telegram\"\n",
        "   }\n",
        "  },\n",
        "  {\n",
        "   \"update_id\": 126178062,\n",
        "   \"message\": {\n",
        "    \"message_id\": 283,\n",
        "    \"from\": {\n",
        "     \"id\": 5154776950,\n",
        "     \"is_bot\": false,\n",
        "     \"first_name\": \"Eder\",\n",
        "     \"language_code\": \"pt-br\"\n",
        "    },\n",
        "    \"chat\": {\n",
        "     \"id\": -1002096546090,\n",
        "     \"title\": \"Eder_IDBot\",\n",
        "     \"type\": \"supergroup\"\n",
        "    },\n",
        "    \"date\": 1715387622,\n",
        "    \"text\": \"Todas as mensagens est\\u00e3o automaticamente sendo salvas no nosso DataLake\"\n",
        "   }\n",
        "  }\n",
        " ]\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c302521c",
      "metadata": {
        "id": "c302521c",
        "papermill": {
          "duration": 0.011599,
          "end_time": "2023-12-10T19:30:49.770874",
          "exception": false,
          "start_time": "2023-12-10T19:30:49.759275",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### 3.1 O que é ETL ?\n",
        "É o processo de extrair, limpar e organizar os dados de uma origem para serem carregados, ou armazenados, em um local específico. Essa etapa é crucial para assegurar que os dados estejam em uma forma apropriada e prontos para análise antes de serem empregados em relatórios, visualizações ou em outros procedimentos analíticos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b66586c7",
      "metadata": {
        "id": "b66586c7",
        "papermill": {
          "duration": 0.011933,
          "end_time": "2023-12-10T19:30:49.794869",
          "exception": false,
          "start_time": "2023-12-10T19:30:49.782936",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### 3.2 Extração\n",
        "Envolve a extração, ou coleta, de dados brutos de diferentes fontes, como as APIs neste caso.\n",
        "\n",
        "#### 3.2.1 AWS Lambda\n",
        "O AWS Lambda é um serviço de computação que permite a execução de código sem a necessidade de gerenciar servidores, também chamado de *serverless*. Nela é possível criar funções sem se preocupar com o escalamento e infraestrutura subjacente. Neste projeto ela se encarregará de executar scripts na linguagem *Python* para manuseio e transformação dos dados.\n",
        "\n",
        "\n",
        "#### 3.2.2 AWS S3\n",
        "O *Amazon S3*, ou *Simple Storage Service*, é um serviço de armazenamento que permite armazenar e recuperar dados de maneira fácil e escalável. É amplamente utilizado para armazenar arquivos, fazer backup e hospedar sistemas online, proporcionando uma solução eficiente e confiável para necessidades de armazenamento na nuvem. Neste projeto, ele armazenará tanto os dados brutos servindo como *Datalake*, quanto os dados transformados e prontos para análise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0642248f",
      "metadata": {
        "id": "0642248f",
        "papermill": {
          "duration": 0.011798,
          "end_time": "2023-12-10T19:30:50.02674",
          "exception": false,
          "start_time": "2023-12-10T19:30:50.014942",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "##### 3.2.3 Extração Telegram\n",
        "Abaixo segue o script em Python usado para a extração dos dados do Telegram.\n",
        "\n",
        "```python\n",
        "import boto3\n",
        "\n",
        "\n",
        "def lambda_handler(event: dict, context: dict) -> dict:\n",
        "\n",
        "  '''\n",
        "  Recebe uma mensagens do Telegram via AWS API Gateway, verifica no\n",
        "  seu conteúdo se foi produzida em um determinado grupo e a escreve,\n",
        "  em seu formato original JSON, em um bucket do AWS S3.\n",
        "  '''\n",
        "\n",
        "  # vars de ambiente\n",
        "\n",
        "  BUCKET = os.environ['AWS_S3_BUCKET']\n",
        "  TELEGRAM_CHAT_ID = int(os.environ['TELEGRAM_CHAT_ID'])\n",
        "\n",
        "  # vars lógicas\n",
        "\n",
        "  tzinfo = timezone(offset=timedelta(hours=-3))\n",
        "  date = datetime.now(tzinfo).strftime('%Y-%m-%d')\n",
        "  timestamp = datetime.now(tzinfo).strftime('%Y%m%d%H%M%S%f')\n",
        "\n",
        "  filename = f'{timestamp}.json'\n",
        "\n",
        "  # código principal\n",
        "\n",
        "  client = boto3.client('s3')\n",
        "\n",
        "  try:\n",
        "\n",
        "    message = json.loads(event[\"body\"])\n",
        "    chat_id = message[\"message\"][\"chat\"][\"id\"]\n",
        "\n",
        "    if chat_id == TELEGRAM_CHAT_ID:\n",
        "\n",
        "      with open(f\"/tmp/{filename}\", mode='w', encoding='utf8') as fp:\n",
        "        json.dump(message, fp)\n",
        "\n",
        "      client.upload_file(f'/tmp/{filename}', BUCKET, f'telegram/context_date={date}/{filename}')\n",
        "\n",
        "  except Exception as exc:\n",
        "      logging.error(msg=exc)\n",
        "      return dict(statusCode=\"500\")\n",
        "\n",
        "  else:\n",
        "      return dict(statusCode=\"200\")\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab3291fb",
      "metadata": {
        "id": "ab3291fb",
        "papermill": {
          "duration": 0.01194,
          "end_time": "2023-12-10T19:30:50.092367",
          "exception": false,
          "start_time": "2023-12-10T19:30:50.080427",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Note que em ambas as extrações os dados provenientes da *API* em formato *JSON* são direcionados para um endereço na *web* armazenado na variável `BUCKET`. Este é o nome do espaço de armazenamento do serviço *AWS S3* para onde estamos enviando os dados brutos. Na sequência será explanado a etapa seguinte, a de transformação dos dados extraídos.\n",
        "\n",
        "Nesta etapa, os dados coletados são processados e transformados para atender aos requisitos do destino. Isso inclui limpeza, filtragem, agregação e qualquer manipulação necessária."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5908922",
      "metadata": {
        "id": "a5908922",
        "papermill": {
          "duration": 0.012136,
          "end_time": "2023-12-10T19:30:51.094547",
          "exception": false,
          "start_time": "2023-12-10T19:30:51.082411",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "#### 3.3.1 Transformação Telegram (Lambda)\n",
        "O script abaixo, de forma similar, recebe os dados brutos de um *bucket* e retira dele somente as informações de interesse, devolvendo o dado estruturado no formato de tabela *parquet*. A diferença entre os scripts da plataforma *Whatsapp* e *Telegram* é basicamente a estrutura em que o dado é recebido via *API*, sendo o do *Whatsapp* maior e com mais aninhamento entre itens.\n",
        "\n",
        "```python\n",
        "\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "import boto3\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "\n",
        "def lambda_handler(event: dict, context: dict) -> bool:\n",
        "\n",
        "  '''\n",
        "  Diariamente é executado para compactar as diversas mensagensm, no formato\n",
        "  JSON, do dia anterior, armazenadas no bucket de dados cru, em um único\n",
        "  arquivo no formato PARQUET, armazenando-o no bucket de dados enriquecidos\n",
        "  '''\n",
        "\n",
        "  # vars de ambiente\n",
        "\n",
        "  RAW_BUCKET = os.environ['AWS_S3_BUCKET']\n",
        "  ENRICHED_BUCKET = os.environ['AWS_S3_ENRICHED']\n",
        "\n",
        "  # vars lógicas\n",
        "\n",
        "  tzinfo = timezone(offset=timedelta(hours=-3))\n",
        "  date = (datetime.now(tzinfo) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "  timestamp = datetime.now(tzinfo).strftime('%Y%m%d%H%M%S%f')\n",
        "\n",
        "  # código principal\n",
        "\n",
        "  table = None\n",
        "  client = boto3.client('s3')\n",
        "\n",
        "  try:\n",
        "\n",
        "      response = client.list_objects_v2(Bucket=RAW_BUCKET, Prefix=f'telegram/context_date={date}')\n",
        "\n",
        "      for content in response['Contents']:\n",
        "\n",
        "        key = content['Key']\n",
        "        client.download_file(RAW_BUCKET, key, f\"/tmp/{key.split('/')[-1]}\")\n",
        "\n",
        "        with open(f\"/tmp/{key.split('/')[-1]}\", mode='r', encoding='utf8') as fp:\n",
        "\n",
        "          data = json.load(fp)\n",
        "          data = data[\"message\"]\n",
        "\n",
        "        parsed_data = parse_data(data=data)\n",
        "        iter_table = pa.Table.from_pydict(mapping=parsed_data)\n",
        "\n",
        "        if table:\n",
        "\n",
        "          table = pa.concat_tables([table, iter_table])\n",
        "\n",
        "        else:\n",
        "\n",
        "          table = iter_table\n",
        "          iter_table = None\n",
        "\n",
        "      pq.write_table(table=table, where=f'/tmp/{timestamp}.parquet')\n",
        "      client.upload_file(f\"/tmp/{timestamp}.parquet\", ENRICHED_BUCKET, f\"telegram/context_date={date}/{timestamp}.parquet\")\n",
        "\n",
        "      return True\n",
        "\n",
        "  except Exception as exc:\n",
        "      logging.error(msg=exc)\n",
        "      return False\n",
        "\n",
        "def parse_data(data: dict) -> dict:\n",
        "\n",
        "  date = datetime.now().strftime('%Y-%m-%d')\n",
        "  timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "  parsed_data = dict()\n",
        "\n",
        "  for key, value in data.items():\n",
        "\n",
        "      if key == 'from':\n",
        "          for k, v in data[key].items():\n",
        "              if k in ['id', 'is_bot', 'first_name']:\n",
        "                parsed_data[f\"{key if key == 'chat' else 'user'}_{k}\"] = [v]\n",
        "\n",
        "      elif key == 'chat':\n",
        "          for k, v in data[key].items():\n",
        "              if k in ['id', 'type']:\n",
        "                parsed_data[f\"{key if key == 'chat' else 'user'}_{k}\"] = [v]\n",
        "\n",
        "      elif key in ['message_id', 'date', 'text']:\n",
        "          parsed_data[key] = [value]\n",
        "\n",
        "  if not 'text' in parsed_data.keys():\n",
        "    parsed_data['text'] = [None]\n",
        "\n",
        "  return parsed_data\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd584d81",
      "metadata": {
        "id": "fd584d81",
        "papermill": {
          "duration": 0.012243,
          "end_time": "2023-12-10T19:30:51.230974",
          "exception": false,
          "start_time": "2023-12-10T19:30:51.218731",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "O scripts retorna a um *bucket*, uma tabela no formato *parquet* organizada com os seguintes dados:\n",
        "\n",
        "\n",
        "\n",
        "* Telegram Schema\n",
        "\n",
        "| Column Name      | Data Type |\n",
        "|------------------|-----------|\n",
        "| message_id       | int64     |\n",
        "| user_id          | int64     |\n",
        "| user_is_bot      | bool      |\n",
        "| user_first_name  | object    |\n",
        "| chat_id          | int64     |\n",
        "| chat_type        | object    |\n",
        "| date             | int64     |\n",
        "| text             | object    |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77ea4789",
      "metadata": {
        "id": "77ea4789",
        "papermill": {
          "duration": 0.012108,
          "end_time": "2023-12-10T19:30:51.255598",
          "exception": false,
          "start_time": "2023-12-10T19:30:51.24349",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### 3.4 Carregamento\n",
        "\n",
        "#### 3.4 AWS Event Bridge\n",
        "É um serviço que permite a criação de regras, muitas vezes baseadas em cronogramas, para acionar eventos específicos, facilitando a automação de tarefas recorrentes. Essa funcionalidade é útil para programar a execução de determinadas ações, como a ativação das funções Lambda deste projeto, conforme a agenda predefinida.\n",
        "\n",
        "\n",
        "\n",
        "O *AWS Event Bridge* foi responsável por inicializar o processo de transformação, uma vez ao dia às 05:00h. O scripts Python de transformação é executado.\n",
        "\n",
        "![Eventos](https://github.com/edersonss1987/AutomacaoTelegram/blob/main/imagens/agendamento%20das%20execu%C3%A7%C3%B5es.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.4 AWS S3\n",
        "Após a execução automazida os aquivos já possivelmente foram salvos e armazenados em um novo `BUCKET`, já com os dados prontos para consumo de geração de insights.\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/edersonss1987/AutomacaoTelegram/blob/main/imagens/arquivos%20salvos%20em%20parquet.png?raw=truee\">"
      ],
      "metadata": {
        "id": "uMsXkSbiPWbV"
      },
      "id": "uMsXkSbiPWbV"
    },
    {
      "cell_type": "markdown",
      "id": "c8f581ec",
      "metadata": {
        "id": "c8f581ec",
        "papermill": {
          "duration": 0.012194,
          "end_time": "2023-12-10T19:30:51.280214",
          "exception": false,
          "start_time": "2023-12-10T19:30:51.26802",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "<a id='apres'></a>\n",
        "## 4. Apresentação\n",
        "Nesta fase os dados são disponibilizados para os usuários finais. Geralmente, as informações são acessadas por meio de ferramentas de consulta, como *SQL*, sendo esta a principal interface para a maioria dos usuários. Nesse contexto, a etapa de apresentação utiliza o *AWS Athena*, uma ferramenta com motor de consulta *SQL*, simplificando a leitura e visualização dos dados armazenados na camada *ETL* para análises eficazes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dded076",
      "metadata": {
        "id": "2dded076",
        "papermill": {
          "duration": 0.012247,
          "end_time": "2023-12-10T19:30:51.304881",
          "exception": false,
          "start_time": "2023-12-10T19:30:51.292634",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### 4.1 AWS Athena\n",
        "O *AWS Athena* é um serviço de consulta interativa que permite analisar dados armazenados no Amazon S3 usando SQL padrão. Ele elimina a necessidade de carregar dados para um banco de dados permitindo explorar grandes conjuntos de dados de maneira fácil e flexível, obtendo insights valiosos sem a necessidade de infraestrutura prévia ou complexos processos de gerenciamento de dados. É especialmente útil em cenários de *Big Data* e *Data Lakes*, como no nosso caso, proporcionando uma abordagem ágil para análise de dados na nuvem. É o meio escolhido para este projeto, para visualizar e analisar as informações armazenadas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3872784f",
      "metadata": {
        "id": "3872784f",
        "papermill": {
          "duration": 0.012254,
          "end_time": "2023-12-10T19:30:51.329694",
          "exception": false,
          "start_time": "2023-12-10T19:30:51.31744",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Na etapa de **apresentação**, o *AWS Athena* tem função de entregar o dados através de uma interface apropriada para os usuários do sistema analítico. Por tanto foi criado, uma \"tabela\", com nome de `telegram`, para  analisados brevemente as informações obtidas. Para a criação das tabelas foi usado a linguagem *SQL*:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "393e48d4",
      "metadata": {
        "id": "393e48d4",
        "papermill": {
          "duration": 0.012262,
          "end_time": "2023-12-10T19:30:51.379319",
          "exception": false,
          "start_time": "2023-12-10T19:30:51.367057",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Criação da tabela SQL - Telegram"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bb6c283",
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2023-12-10T18:53:53.151854Z",
          "iopub.status.idle": "2023-12-10T18:53:53.152845Z",
          "shell.execute_reply": "2023-12-10T18:53:53.152626Z",
          "shell.execute_reply.started": "2023-12-10T18:53:53.152595Z"
        },
        "id": "6bb6c283",
        "papermill": {
          "duration": 0.01217,
          "end_time": "2023-12-10T19:30:51.4039",
          "exception": false,
          "start_time": "2023-12-10T19:30:51.39173",
          "status": "completed"
        },
        "tags": [],
        "vscode": {
          "languageId": "sql"
        }
      },
      "source": [
        "```sql\n",
        "CREATE EXTERNAL TABLE `telegram`(\n",
        "  `message_id` bigint,\n",
        "  `user_id` bigint,\n",
        "  `user_is_bot` boolean,\n",
        "  `user_first_name` string,\n",
        "  `chat_id` bigint,\n",
        "  `chat_type` string,\n",
        "  `text` string,\n",
        "  `date` bigint)\n",
        "PARTITIONED BY (\n",
        "  `context_date` date)\n",
        "ROW FORMAT SERDE\n",
        "  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
        "STORED AS INPUTFORMAT\n",
        "  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'\n",
        "OUTPUTFORMAT\n",
        "  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
        "LOCATION\n",
        "  's3://<bucket>/telegram/'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após criação da tabela é sempre importante executar o comando abaixo, para informar ao Athena que existem novos arquivos no Bucket, isso irá garantir que os dados sejam lidos e consultados por ele.\n",
        "\n",
        "```sql\n",
        "MSCK REPAIR TABLE `telegram`\n",
        "```"
      ],
      "metadata": {
        "id": "Bi_nAkCOSxAY"
      },
      "id": "Bi_nAkCOSxAY"
    },
    {
      "cell_type": "markdown",
      "id": "7e1692aa",
      "metadata": {
        "id": "7e1692aa",
        "papermill": {
          "duration": 0.012215,
          "end_time": "2023-12-10T19:30:51.42855",
          "exception": false,
          "start_time": "2023-12-10T19:30:51.416335",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "A consulta SQL a seguir foi projetada para validar a criação bem-sucedida de uma das tabelas em nosso banco de dados. Além disso, ela confirma se os dados particionados foram corretamente capturados, armazenados e depois recuperados no serviço S3. Esta verificação é crucial para garantir a integridade dos dados antes de prosseguirmos com qualquer análise adicional."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73fae4d8",
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2023-12-10T18:53:53.15399Z",
          "iopub.status.idle": "2023-12-10T18:53:53.154528Z",
          "shell.execute_reply": "2023-12-10T18:53:53.154273Z",
          "shell.execute_reply.started": "2023-12-10T18:53:53.154246Z"
        },
        "id": "73fae4d8",
        "papermill": {
          "duration": 0.012107,
          "end_time": "2023-12-10T19:30:51.45304",
          "exception": false,
          "start_time": "2023-12-10T19:30:51.440933",
          "status": "completed"
        },
        "tags": [],
        "vscode": {
          "languageId": "sql"
        }
      },
      "source": [
        "```sql\n",
        "SELECT * FROM \"telegram\";\n",
        "```\n",
        "![Resposta SQL](https://github.com/edersonss1987/AutomacaoTelegram/blob/main/imagens/sql1.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3762c4ea",
      "metadata": {
        "id": "3762c4ea",
        "papermill": {
          "duration": 0.012113,
          "end_time": "2023-12-10T19:30:51.502004",
          "exception": false,
          "start_time": "2023-12-10T19:30:51.489891",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### 4.2 Análise Exploratória de Dados\n",
        "Nesta seção, aprofundamos a análise dos dados enriquecidos por meio de consultas, buscando entender melhor as nuances e informações presentes. As consultas SQL feitas no serviço *Athena* nos possibilita a criar análises exploratórias e extração de insights, com essas conclusões podemos entender melhor o comportamento dos usuários do grupo e, a partir disso, criar estratégias para melhorar a experiência deles."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4104c4f5",
      "metadata": {
        "id": "4104c4f5",
        "papermill": {
          "duration": 0.012318,
          "end_time": "2023-12-10T19:30:51.575969",
          "exception": false,
          "start_time": "2023-12-10T19:30:51.563651",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "A seguir, uma consulta SQL realizada para uma análise dos dados enriquecidos da tabela `telegram`. Inicialmente, converte o campo de data/hora (`timestamp`) para um formato legível de carimbo de data/hora. Posteriormente, extrai informações como a hora do dia, o dia da semana e o número da semana a partir do carimbo de data/hora. Por fim, agrupa os dados, contabilizando o número de mensagens para cada combinação de hora, dia da semana e número da semana. Isso proporciona um resumo da frequência das mensagens, destacando padrões temporais relevantes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dfe3b2c",
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2023-12-10T18:53:53.156728Z",
          "iopub.status.idle": "2023-12-10T18:53:53.157277Z",
          "shell.execute_reply": "2023-12-10T18:53:53.157036Z",
          "shell.execute_reply.started": "2023-12-10T18:53:53.157008Z"
        },
        "id": "1dfe3b2c",
        "papermill": {
          "duration": 0.012162,
          "end_time": "2023-12-10T19:30:51.600677",
          "exception": false,
          "start_time": "2023-12-10T19:30:51.588515",
          "status": "completed"
        },
        "tags": [],
        "vscode": {
          "languageId": "sql"
        }
      },
      "source": [
        "```sql\n",
        "WITH\n",
        "parsed_date_cte AS (\n",
        "    SELECT\n",
        "        *,\n",
        "        CAST(date_format(from_unixtime(\"date\"),'%Y-%m-%d %H:%i:%s') AS timestamp) AS parsed_date\n",
        "    FROM \"whatsapp\"\n",
        "),\n",
        "hour_week_cte AS (\n",
        "    SELECT\n",
        "        *,\n",
        "        EXTRACT(hour FROM parsed_date) AS hora_do_dia,\n",
        "        EXTRACT(dow FROM parsed_date) AS dia_da_semana,\n",
        "        EXTRACT(week FROM parsed_date) AS semana_do_ano\n",
        "    FROM parsed_date_cte\n",
        ")\n",
        "SELECT\n",
        "    hora_do_dia,\n",
        "    dia_da_semana,\n",
        "    semana_do_ano,\n",
        "    count(1) AS \"qtd_de_mensagens\"\n",
        "FROM hour_week_cte\n",
        "GROUP BY\n",
        "    hora_do_dia,\n",
        "    dia_da_semana,\n",
        "    semana_do_ano\n",
        "ORDER BY\n",
        "    semana_do_ano,\n",
        "    dia_da_semana\n",
        "```\n",
        "![Resposta SQL](https://github.com/edersonss1987/AutomacaoTelegram/blob/main/imagens/sql2.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WNJD2jhjWLbQ"
      },
      "id": "WNJD2jhjWLbQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y_GzhHYfWLTW"
      },
      "id": "y_GzhHYfWLTW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essa consulta SQL está sendo usada para obter a quantidade de mensagens enviadas por cada usuário no aplicativo “telegram” em cada data específica. Aqui está o que cada parte da consulta faz:\n",
        "\n",
        "`SELECT user_id, user_first_name`, `context_date, count(1) AS \"message_amount\"`: Esta parte seleciona as colunas de interesse. A função count(1) conta o número de linhas para cada combinação única de user_id, user_first_name e context_date, e o resultado é rotulado como message_amount.\n",
        "FROM \"telegram\": Esta parte indica que os dados estão sendo retirados da tabela “telegram”.\n",
        "\n",
        "`GROUP BY user_id, user_first_name, context_date`: Esta parte agrupa os dados por user_id, user_first_name e context_date. Isso significa que para cada combinação única dessas três variáveis, a consulta contará o número de linhas (ou seja, o número de mensagens).\n",
        "\n",
        "`ORDER BY context_date DESC`: Esta parte ordena os resultados pela coluna context_date em ordem decrescente, o que significa que as datas mais recentes aparecerão primeiro.\n",
        "Portanto, o propósito desta consulta é fornecer uma visão de quantas mensagens cada usuário enviou em cada data, com os resultados ordenados do mais recente ao mais antigo. Isso pode ser útil para entender o comportamento do usuário ao longo do tempo. Por exemplo, você pode ver se um usuário está se tornando mais ativo ou menos ativo no aplicativo.\n",
        "\n",
        "```sql\n",
        "SELECT\n",
        "  user_id,\n",
        "  user_first_name,\n",
        "  context_date,\n",
        "  count(1) AS \"message_amount\"\n",
        "FROM \"telegram\"\n",
        "GROUP BY\n",
        "  user_id,\n",
        "  user_first_name,\n",
        "  context_date\n",
        "ORDER BY context_date DESC\n",
        "````\n",
        "![Resposta SQL](https://github.com/edersonss1987/AutomacaoTelegram/blob/main/imagens/sql3.png?raw=true)"
      ],
      "metadata": {
        "id": "CdzJDHefWLkq"
      },
      "id": "CdzJDHefWLkq"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zukf_tSfXz88"
      },
      "id": "zukf_tSfXz88",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MetsJBYTXz0N"
      },
      "id": "MetsJBYTXz0N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O propósito desta consulta é fornecer uma visão do comprimento médio das mensagens que cada usuário enviou em cada data, com os resultados ordenados do mais recente ao mais antigo. Isso pode ser útil para entender o comportamento do usuário ao longo do tempo. Por exemplo, você pode ver se um usuário está enviando mensagens mais longas ou mais curtas ao longo do tempo.\n",
        "\n",
        "```sql\n",
        "SELECT\n",
        "  user_id,\n",
        "  user_first_name,\n",
        "  context_date,\n",
        "  CAST(AVG(length(text)) AS INT) AS \"average_message_length\"\n",
        "FROM \"telegram\"\n",
        "GROUP BY\n",
        "  user_id,\n",
        "  user_first_name,\n",
        "  context_date\n",
        "ORDER BY context_date DESC\n",
        "````\n",
        "![Resposta SQL](https://github.com/edersonss1987/AutomacaoTelegram/blob/main/imagens/sql4.png?raw=true)"
      ],
      "metadata": {
        "id": "Y6oUStTWX0bT"
      },
      "id": "Y6oUStTWX0bT"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MbmI_hJNY7KK"
      },
      "id": "MbmI_hJNY7KK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rfc5TYZ7Y7CH"
      },
      "id": "rfc5TYZ7Y7CH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O propósito desta consulta é fornecer uma visão de quantas mensagens foram enviadas no aplicativo “telegram” em cada data, com os resultados ordenados do mais recente ao mais antigo. Isso pode ser útil para entender o volume de mensagens ao longo do tempo. Por exemplo, você pode ver se o volume de mensagens está aumentando ou diminuindo ao longo do tempo.\n",
        "\n",
        "```sql\n",
        "SELECT\n",
        "  context_date,\n",
        "  count(1) AS \"message_amount\"\n",
        "FROM \"telegram\"\n",
        "GROUP BY context_date\n",
        "ORDER BY context_date DESC\n",
        "````\n",
        "![Resposta SQL](https://github.com/edersonss1987/AutomacaoTelegram/blob/main/imagens/sql5.png?raw=true)"
      ],
      "metadata": {
        "id": "w1r_z8xTY7XW"
      },
      "id": "w1r_z8xTY7XW"
    },
    {
      "cell_type": "markdown",
      "id": "2deae23c",
      "metadata": {
        "id": "2deae23c",
        "papermill": {
          "duration": 0.012326,
          "end_time": "2023-12-10T19:30:51.726629",
          "exception": false,
          "start_time": "2023-12-10T19:30:51.714303",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "<a id='conclu'></a>\n",
        "## 5. Conclusão"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98c93549",
      "metadata": {
        "id": "98c93549",
        "papermill": {
          "duration": 0.01223,
          "end_time": "2023-12-10T19:30:51.751338",
          "exception": false,
          "start_time": "2023-12-10T19:30:51.739108",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "A integração do Telegram e a análise de dados com serviços de nuvem, como a AWS, melhoram significativamente as operações de uma empresa. Essas tecnologias permitem que os clientes interajam facilmente com a empresa a qualquer hora, pois estão sempre disponíveis. A AWS ajuda a tornar essa interação ainda melhor, pois permite que a empresa ajuste seus recursos de TI rapidamente, conforme necessário, garantindo que o serviço seja escalável, flexível e confiável.\n",
        "\n",
        "Com a análise de dados realizada na nuvem, as empresas podem processar grandes quantidades de informações de forma eficiente e segura. Isso ajuda a entender melhor os comportamentos e preferências dos clientes, analisando os dados coletados durante as interações com eles.\n",
        "\n",
        "Além disso, usar a nuvem pode reduzir custos, pois não é necessário investir em equipamentos físicos. Também permite que as empresas se adaptem rapidamente a mudanças nas necessidades do mercado, mantendo a eficiência. A segurança é uma prioridade, e serviços como a AWS têm medidas fortes para proteger os dados dos clientes contra ataques online.\n",
        "\n",
        "Em resumo, essa combinação de tecnologias melhora a forma como os clientes usam os aplicativos de mensagem, aumenta a eficiência das operações da empresa e ajuda na tomada de decisões estratégicas. A análise de dados é essencial para as empresas hoje em dia, pois fornece as ferramentas necessárias para inovar e alcançar a excelência no mercado."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30587,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 6.593106,
      "end_time": "2023-12-10T19:30:52.284937",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-12-10T19:30:45.691831",
      "version": "2.4.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}